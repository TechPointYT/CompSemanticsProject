{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIN 373 UT Austin :: Jessy Li\n",
    "\n",
    "## Vectorizing categorical features\n",
    "\n",
    "### The inner workings\n",
    "\n",
    "Let's encode the Naive Bayes example we used in class into the count table shown on the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's the data. Let's pretend these are grammatical sentences.\n",
    "docs_train = [\"Chinese Beijing Chinese\",\n",
    "              \"Chinese Chinese Shanghai\",\n",
    "              \"Chinese Macao\",\n",
    "             \"Tokyo Japan Chinese\"]\n",
    "Y_train = [1, 1, 1, 0]\n",
    "\n",
    "docs_test = [\"Chinese Chinese Chinese Tokyo Japan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chinese', 'Beijing', 'Chinese'], ['Chinese', 'Chinese', 'Shanghai'], ['Chinese', 'Macao'], ['Tokyo', 'Japan', 'Chinese']]\n",
      "[['Chinese', 'Chinese', 'Chinese', 'Tokyo', 'Japan']]\n"
     ]
    }
   ],
   "source": [
    "## first need to tokenize each document\n",
    "docs_train_tokenized = [doc.split() for doc in docs_train]\n",
    "print(docs_train_tokenized)\n",
    "\n",
    "docs_test_tokenized = [doc.split() for doc in docs_test]\n",
    "print(docs_test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we put words into a table?\n",
    "First, we need to create that table. The rows are just the examples. But we need to come up with the columns.\n",
    "We need to assign each word to a column number!\n",
    "We do that by creating a dictionary to map from word to a unique column id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chinese': 0, 'Beijing': 1, 'Shanghai': 2, 'Macao': 3, 'Tokyo': 4, 'Japan': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_col_id = {}\n",
    "for doc in docs_train_tokenized:\n",
    "    for word in doc:\n",
    "        if word not in word_to_col_id:\n",
    "            word_to_col_id[word] = len(word_to_col_id)\n",
    "            \n",
    "print(word_to_col_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make the table, and fill it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 1. 0. 0. 0. 0.]\n",
      " [2. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.zeros((len(docs_train), len(word_to_col_id)))\n",
    "for i,doc in enumerate(docs_train_tokenized):\n",
    "    for word in doc:\n",
    "        col_id = word_to_col_id[word]\n",
    "        X_train[i][col_id] += 1\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for testing docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.zeros((len(docs_test), len(word_to_col_id)))\n",
    "for i,doc in enumerate(docs_test_tokenized):\n",
    "    for word in doc:\n",
    "        col_id = word_to_col_id[word]\n",
    "        X_test[i][col_id] += 1\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if there is a new word in the testing docs?\n",
    "\n",
    "\n",
    "### Using a tool\n",
    "\n",
    "All of this is implemented by sklearn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), which does tokenization AND the corpus-to-table transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorizer = vectorizer.fit_transform(docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beijing', 'chinese', 'japan', 'macao', 'shanghai', 'tokyo']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0 0 0 0]\n",
      " [0 2 0 0 1 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectorizer.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t2\n",
      "  (0, 0)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 4)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "## vectorizer uses sparse encoding\n",
    "print(X_train_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now: Naive Bayes\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we set alpha to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jl67946/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "model2 = MultinomialNB(alpha = 0)\n",
    "model2.fit(X_train, Y_train)\n",
    "print(model2.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
